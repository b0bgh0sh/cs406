{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bob Ghosh, 42039157, j0k0b\n",
    "# Assignment 4, CPSC 406, March 16. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, ForwardDiff, Optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof by Induction:\n",
    "Let <B>n = 1</B>; then $S = S_1$. So, if $S_1$is a convex set, then by definition, S is also a convex set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us suppose <B>n = 2</B>,  then $S = S_1 \\cap S_2$. Furthermore, let us assume there exists two points a and b, such that $a,b \\in S_1 \\cap S_2$, that is a, b are the two common points within $S_1$ and $S_2$.\n",
    "\n",
    "If we consider a line segment ab, connecting the points a and b, then $ab \\in S_1$ and $ab \\in S_2$, by the definition of convexity. Thus $ab \\in S_1 \\cap S_2$, and hence $S$ is a convex set. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let us consider that S is a convex set for <B>n=k</B> intersecting sets. We now need to prove that S is a convex set for k+1 intersecting sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, let us set $S_{k^*} = S_1 \\cap S_2 \\cap \\dots S_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For n = k+1, $S_{k^*+1} = S_1 \\cap S_2 \\cap \\dots S_k \\cap S_{k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$=> S_{k^*+1} = S_{k^*} \\cap S_{k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our assumption, we know that $S_{k^*}$ is a convex set. And from [1], we see that intersection of two convex sets is a convex set. So, now we can claim that $S_{k^*+1}$ is a convex set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we have proved by induction that the intersection of convex set is itself a convex set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given $S = \\{(x,t):||x||_2 \\leq t\\}$. \n",
    "Therefore, we need to pick two points $x_1$ and $x_2$ such that $||x_1||_2 \\leq t$ and $||x_2||_2 \\leq t$. <br><br>\n",
    "$=> \\theta ||x_1||_2 \\leq \\theta t$ [1]<br><br> and $(1- \\theta)||x_2||_2 \\leq (1- \\theta)t$ [2]<br><br> for some $\\theta \\in [0,1]$.<br><br>\n",
    "\n",
    "On adding [1] and [2], we get -> <br>\n",
    "$\\theta ||x_1||_2 + (1-\\theta)||x_2||_2 \\leq \\theta t + (1-\\theta)t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which becomes: $\\theta ||x_1||_2 + (1-\\theta)||x_2||_2 \\leq  t$, thus by the definition of convexity, S is a convex set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Nonnegative Constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof by Contradiction\n",
    "So, in this case, we are assuming (2): $x^* = proj_c(x^* - \\gamma \\nabla f(x^*))$ for any constant $\\gamma > 0$. Furthermore, projection itself is an optimization problem: $proj_c(z) = \\underset x {argmin}\\  \\ ||x-z||_2^2$ subject to $x \\in C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $C = \\{x:x_i \\geq 0\\}$, as a point of contradiction, let us assume $\\nabla f(x^*) < 0$. In (2) we say that $\\gamma > 0$ for $\\epsilon > 0$, therefore, $x^* = \\underset {x \\geq 0} {argmin} \\ \\ ||x-x^*-\\epsilon||_2^2$. Thus optimally, we would find that $x = x^* + \\epsilon$. Thus, there is a contradiction, as each term would not be non-negative. Therefore, $\\nabla f(x^*)$ has to be $\\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Normal Cone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) states that $\\nabla f(x^*)^T(y-x^*) \\geq 0, \\forall y \\in C$. We will be using (2) to show equivalency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From (2), $x^* = \\underset x {argmin} \\ \\ ||x-x^* + \\gamma \\nabla f(x^*)||_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\underset x {argmin} \\ \\ (x^T-(x^*)^T + \\gamma \\nabla f(x^*)^T(x - x^* + \\gamma \\nabla f(x^*)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\underset x {argmin} \\ \\ (x^Tx - 2x^Tx^* + 2 \\gamma x^T \\nabla f(x^*) + (x^*)^Tx^*-2 \\gamma \\nabla f(x^*)^Tx^*)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\underset x {argmin} \\ \\ (||x-x^*||_2^2 + 2 \\gamma \\nabla f(x^*)^T(x-x^*))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we know that $||x-x^*||_2^2 \\geq 0$, $f(x^*) \\geq 0$ and $x-x^* \\geq 0$. So, all the terms in the above equation are positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use $\\nabla f(x^*)^T(y-x^*) < 0$ to prove that $x^* \\neq \\underset x {argmin} \\ \\ (||x-x^*||_2^2 + 2 \\gamma \\nabla f(x^*)^T(x-x^*))$, by using $\\gamma$ big enough and making the term $argmin < 0$. Hence, that shows the equivalency between (3) and (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã—1 Array{Float64,2}:\n",
       " 0.08333333250256361\n",
       " 0.3333333327835978 \n",
       " 0.5833333399671755 "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Projected Gradient Descent\n",
    "A = [1 1 1; 2 1 0];\n",
    "b = [1;0.5];\n",
    "alpha = 0.01; # step-size\n",
    "x_temp = zeros(size(A)[2],1);\n",
    "x_0 = zeros(size(A)[2],1);\n",
    "\n",
    "function gradient(x, A, b)\n",
    "    f(x)= 100.0 * (x[2] - x[1]^2)^2 + (1-x[1])^2 + 100.0 * (x[3]-x[2]^2)^2+(1-x[2])^2\n",
    "    return ForwardDiff.gradient(f,x)\n",
    "end\n",
    "\n",
    "function gradientDescent(A, b, x_temp, alpha, n)    \n",
    "    m = length(b)\n",
    "    for iter in 1:100\n",
    "        g = gradient(x_temp, A, b)\n",
    "        x_before = A\\b\n",
    "        x_temp = x_temp-alpha * g\n",
    "        fun(x_temp) = norm((x_before - x_temp), 2)\n",
    "        res = Optim.optimize(fun, x_temp)\n",
    "        x_temp = Optim.minimizer(res)\n",
    "    return x_temp\n",
    "    end\n",
    "end\n",
    "\n",
    "Theta = gradientDescent(A, b, x_temp, alpha, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.08333333333333334\n",
       " 0.3333333333333333 \n",
       " 0.5833333333333336 "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reduced Gradient Method\n",
    "N = 3;\n",
    "f3(x)= 100.0 * (x[2] - x[1]^2)^2 + (1-x[1])^2 + 100.0 * (x[3]-x[2]^2)^2+(1-x[2])^2;\n",
    "g(x) = ForwardDiff.gradient(f3, x);\n",
    "H(x) = ForwardDiff.hessian(f3, x);\n",
    "x = [1,1,1];\n",
    "g(x);\n",
    "cholesky(H(x));\n",
    "a1 = ones(1,3) ; b1 = 1;\n",
    "a2 = [2 1 0]; b2 = 0.5;\n",
    "A = vcat(a1,a2); b = vcat(b1,b2);\n",
    "Q, R = qr(A'); Z = reshape(Q[:,3],3,1);\n",
    "xbar = A \\ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.708222277663692e-9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(Theta - xbar, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solutions are not strictly equal, although they are somewhat similar, depending on the degree of accuracy. Perhaps, more iterations with different step size could lead to a better, or even same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given that $f(x) = -\\sum_{i=1}^n x_i log(x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can derive $\\nabla f(x)$ as $\\nabla f(x) = -(log(x_i) + 1)$. And from this, we can find the Hessian to be $H = \\begin{bmatrix} \n",
    "-1/x_1 & 0 & \\dots & 0 \\\\\n",
    "0 & -1/x_2 & \\dots & 0\\\\\n",
    "\\vdots & \\ddots & \\dots & \\dots \\\\\n",
    "0 & 0 & \\dots & -1/x_n \\\\\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\therefore H = (-1/x_i) I_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also know that $x \\in [0,1]$, so H is a negative definite matrix. And hence, f(x) is strictly concave when $x \\neq y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Log-Sum-Exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given $f(x) = \\log(\\sum_{i=1}^m e^{a_i^T x})$. For simplicity, let us define $u_i =  e^{a_i^Tx}$ and $w_i = e^{a_i^Ty}$. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the nature of convexity, we can claim that $f(\\theta x + (1-\\theta)y) = \\log(\\sum_{i=1}^n e^{\\theta a_i^T x + (1-\\theta)a_i^Ty})$ [2] for some $\\theta \\in [0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [1] and [2], we can simplify the equation as $\\log(\\sum_{i=1}^n e^{\\theta a_i^T x + (1-\\theta)a_i^Ty}) = \\log(\\sum_{i=1}^n u_i^{\\theta}w_i^{1-\\theta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the properties of inequality we can state that: <br>\n",
    "$\\log(\\sum_{i=1}^n u_i^{\\theta}w_i^{1-\\theta}) \\leq \\log((\\sum_{i=1}^n u_i)^{\\theta}(\\sum_{i=1}^n w_i)^{1-\\theta})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\leq \\theta \\log \\sum_{i=1}^n u_i + (1-\\theta) \\log \\sum_{i=1}^n w_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brings us to $f(\\theta x + (1- \\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y)$, which proves the convexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
